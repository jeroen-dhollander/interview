-----------------
Data Structures
-----------------

 Array
 -----
     + O(1) lookup using index
     + Space efficient
     + Memory locality
     - Fixed size (or put otherwise, adding elements is expensive)
         -> filling an array with n elements is amortized O(n)
     - insert/delete is O(n) as it requires moving of elements
     + append/pop is O(1)

 Linked List
 -----------
     - O(n) lookup
     - Wastes 1/2 pointers per element
     - No memory locality
     - No random access
     + insert/delete/swap is O(1)
     + append/pop is O(1)

 Self organizing list
 --------------------
    * Whenever a key is accessed, move it to the front
    * Most applications have an uneven access frequency and locality of reference
    * Works with linked-list, arrays, trees

 Stack
 -----
     * LIFO
     * Simple and efficient
     * Great when order doesn't matter
     * Linked list or array (is size known up front?)

 Queue
 -----
     * FIFO
     * Minimizes maximum wait time
     - Harder to implement than stack
     * Linked list or array (is size known up front?)

 Dictionary
 ----------
     * Key/Value lookup
     * Can be implemented using
         - sorted/unsorted array/linked-list
         - binary search tree
         - hash map

 Binary Search Tree
 ------------------
     + O(lg n) lookup (actually O(height), which is 'lg n' if balanced)
     - Wastes 2/3 pointers (left, right, [parent]) per element
     - No memory locality
     + insert/delete is O(height)

 B-tree
 ------
    * Use when data does not fit in main memory

 Priority Queue
 --------------
     * Operations insert, find-min or find-max, delete-min or delete-max
     * Implemented using a max/min heap

   Bounded height priority queue
   -----------------------------
      * AKA bucket queue
      * Useful if the keys are integers between 1 and n (think a lot of graph problems)
      * Have array of 'n' linked lists, and a pointer to the top one with data
      * All elements in linked list 'x' have the same priority 'x'.

   Fibonacci and pairing heaps
   ---------------------------
      * Fancy, but allow efficient decrease-key operations

 Hash Map
 --------
     * Great for 100 to 10M keys
     * 2 options for collisions: Chaining vs Open Addressing
                                 (first open spot)
         --> delete might mean moving, search needs to keep going
         + Open-Addressing has better locality
         - Open-Addressing might perform worse if it fills up
     * Number of buckets 'm' should be about the same as maximum 'n'
     * For open-addressing --> add 30%
     * 'm' should be prime for best hashing performance
     * Print statistics for key distribution to ensure you use the right hash

 Max Heap
 --------
     * Keeps partial ordering
     * Each node is always bigger than its 2 children
     * It's a full binary tree
       (with empty spots only at leaf level to the right)
     * Insert:
         - Insert element in first empty spot
         - Bubble up: while element is bigger than its parent:
                          swap with parent
     * Extract-max:
         - Replace root with the rightmost leaf node!!!
         - Bubble down: while element is smaller than any of its children:
                            swap with biggest child
     * Both operations are O(log n)
     * Populating it is O(n):
         - Add all elements randomly
         - Then for each element from end to start: do bubble_down
           (which will be cheap as the height of the subtree is small)
     * Stored as fixed-size array (use 1 based indexing for ease of reasoning)
     * NOTE: How to insert in a full tree: you don't.
             To keep the 'n' smallest items, use a max-heap and pop root if
             the new element is smaller.

 Min Heap
 --------
     * Same but min is at the top

 Set
 ---
    * When:
        - Fixed universe of items
        - Split in subsets where order does not matter
    * Operations:
        - Membership check
        - Compute union/intersection
        - Add/Remove members
    * Note: Storing sets sorted makes a lot of these operations perform in linear time
    * Implementations:
        - Bit vectors
            + space efficient
            + cheap add/remove/intersection/union
            - Not great for sparse sets, as enumerating members always takes O(n)
        - Sorted Array, dictionary
        - Bloom filter:
            * If universe is not fixed, hash values and use bitvector
            * Use multiple hashes/bitvectors to statistically remove collisions
            * Use for membership tests
            * Note: will always have a small chance of error (false positives),
                    but a lot of things like spellcheckers do not care.
        - Disjoint-set (see below):
           + If all sets are disjoint

 Disjoint-Set
 ------------
    * A collection of disjoint sets
    * Can divide a collection (graph) in components
    * Implementation: Parent array where Parent[i] is the parent of the i-th element, or 'i' itself if 'i' is the root parent of a subset.
    * O(log n) merge sets
    * O(log n) check if 2 elements are in the same component
      * Each subset is a tree with parent pointers
      * We also know the number of elements in the tree
      * To check if 2 elements are in the same component compare the roots
      * To merge 2 components, make the smallest a subtree of the largest (to prevent worst case below).
    * Worst case: everything ends up as a linked list.
        * optimization: when you 'find()' the root of node 'i', update parent pointer of 'i' to point directly to its root.

  Suffix trees
  ------------
      * Find all places where string S occurs
      * O(n) lookup
      + Cheap to build/update
      - Uses lots of memory
      - Hard to construct efficiently
      * Trie (=prefix tree) of the n suffixes of a n-character string s.
        For example the below tree contains the words 'the', 'there' and 'them'
        (The '*' indicates that node also serves as an endpoint for 'the')

                       < >
                        |
                        T
                        |
                        H
                        |
                        E(*)
                   R----+----M
                   |
                   E
     * Alternatively: Add non-printable character not in any of the strings
                      to end of all strings, Then 'the<end>' will end in a leaf.
     * O(n^2) space and time to build
     * More efficient O(n) space: Have a single element represent a range:

                       < >
                        |
                       THE(*)
                   RE---+----M
     * Uses:
         - Longest common substring
             - Build suffix tree with all suffixes of all strings.
               On each node track 1) length of common prefix and 2) number of
               distinct strings under it
             --> Find substring in linear time

  Suffix array
  ------------
     * Easier and less memory consumption than suffix tree (factor 4 smaller)
     * Sorted array of the suffixes of a string, so for 'data' the array will
       contain [ 'a', 'ata', 'data', 'ta' ].
       But instead of containing these suffixes it contains their position in the
       string [ '3', '1', '0', '2' ] (making it only use O(n) memory).
     * Because it is sorted it is easy to for example find all occurrences of a
       given pattern. In or example if we look for the pattern 'a' we
          - do a binary search pointing us to the first element in the array
          - then walk all the options until we no longer have a prefix match
     * Note: Fancy algorithms exist to populate this efficiently

  Tree rotations
  --------------
     * Modify a binary search tree so one of the children becomes the root
     * This is used while balancing binary trees
     * This is easiest to visualize by giving each child a value
       that satisfies the binary tree requirements.
     * We can rotate 'right' and 'left'

                4      Right rotate       2
               / \     makes '2'         / \
              2   5    the root         1   4
             / \                           / \
            1   3      +---------->       3   5


                4      Left rotate        2
               / \     restores '4'      / \
              2   5    as the root      1   4
             / \                           / \
            1   3      <----------+       3   5


  AVL-Tree
  --------
     * A balanced binary tree
     * For each node we ensure the height of its subtrees are at most 1 different.
     * Algorithm:
        - On each node, remember its height
          (note you can save memory by storing '0' if its balanced,
           '-1' if the left is one higher and '1' if the right is higher,
           but the algorithm is a bit easier to understand if we remember the height,
           and it can be modified later)
       - After inserting a node we travel up to the root and check if the tree
         is still balanced. If it is not we will rebalance the tree.
       - How unbalanced can a node be?
          --> it will have a height difference of 2 between its subtrees,
              as we only added a single element and the tree was previously balanced
              meaning no height difference exceeded 1.
      - So this means we only have 4 possible case:
         * In all the examples below, T1, T2, T3 and T4 have height H,
         * The names are 'G' for Grandparent, 'P' for Parent and 'C' for child.
         * 'G' is the node that is unbalanced.
         1) the left-left subtree is heaviest
             --> we can fix this with a single right rotate of G

                   G                              P
                  / \       Right Rotate        /   \
                 P   T4                        C      G
                / \        +------------>    /  \    /  \
               C   T3                       T1  T2  T3  T4
              / \
            T1   T2

         2) the left-right subtree is heaviest
             --> We first do a left rotate of C
             --> And then it becomes the same as case 1. so another right rotate
                 of G balances the tree

                   G                            G
                  / \       Left Rotate C      / \
                 P   T4                       C   T4
                / \        +------------>    / \
               T3  C                        P   T2
                  / \                      / \
                T1   T2                  T1   T3

          3) the right-right subtree is heaviest
             This is the mirror of case 1., so a left rotate of G fixes the balance.

          4) the right-left subtree is heaviest
             This is the mirror of case 2., so a right rotate of C followed
             by a left rotate of G does the trick.
